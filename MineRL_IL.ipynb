{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Imitation Learning in MineRL\n",
    "This tutorial contains a simple example of how to build a imitation-learning based agent that can solve the MineRLNavigateDense-v0 environment. For more information about that environment, see this [MineRL Docs](http://minerl.io/docs/environments/index.html#minerlnavigatedense-v0).\n",
    "\n",
    "For more Imitation Learning algorithms, like a Dagger in Tensorflow, see that Github repo, [Dagger](https://github.com/zsdonghao/Imitation-Learning-Dagger-Torcs).\n",
    "\n",
    "Parts of this tutorial are based on code by [Arthur Juliani](https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kimbring2/minerl_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/kimbring2/minerl_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/kimbring2/minerl_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/kimbring2/minerl_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/kimbring2/minerl_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/kimbring2/minerl_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "try:\n",
    "    xrange = xrange\n",
    "except:\n",
    "    xrange = range\n",
    "    \n",
    "env_name = 'MineRLTreechop-v0'\n",
    "data_path = '/media/kimbring2/6224AA7924AA5039/minerl_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the CartPole Environment\n",
    "If you don't already have the OpenAI gym installed, use  `pip install gym` to grab it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we try running the environment with random actions? How well do we do? (Hint: not so well.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the task is to achieve a reward of 200 per episode. For every step the agent keeps the pole in the air, the agent recieves a +1 reward. By randomly choosing actions, our reward for each episode is only a couple dozen. Let's make that better with RL!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up our Neural Network agent\n",
    "This time we will be using a Policy neural network that takes observations, passes them through a single hidden layer, and then produces a probability of choosing a left/right movement. To learn more about this network, see [Andrej Karpathy's blog on Policy Gradient networks](http://karpathy.github.io/2016/05/31/rl/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kimbring2/minerl_env/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/kimbring2/minerl_env/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "convFlat: Tensor(\"Flatten/flatten/Reshape:0\", shape=(?, 57600), dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-2-945474e8b59f>:30: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "H = 57600\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "state = tf.placeholder(shape=[None,64,64,4], dtype=tf.float32)\n",
    "conv1 = slim.conv2d( \\\n",
    "            inputs=state,num_outputs=16,kernel_size=[2,2],stride=[1,1],padding='VALID', \n",
    "                    biases_initializer=None,activation_fn=tf.nn.relu)\n",
    "conv2 = slim.conv2d( \\\n",
    "            inputs=conv1,num_outputs=16,kernel_size=[2,2],stride=[1,1],padding='VALID', \n",
    "                    biases_initializer=None,activation_fn=tf.nn.relu)\n",
    "conv3 = slim.conv2d( \\\n",
    "            inputs=conv2,num_outputs=16,kernel_size=[2,2],stride=[1,1],padding='VALID', \n",
    "                    biases_initializer=None,activation_fn=tf.nn.relu)\n",
    "conv4 = slim.conv2d( \\\n",
    "            inputs=conv3,num_outputs=16,kernel_size=[2,2],stride=[1,1],padding='VALID', \n",
    "                    biases_initializer=None,activation_fn=tf.nn.relu)\n",
    "\n",
    "convFlat = slim.flatten(conv4)\n",
    "print(\"convFlat: \" + str(convFlat))\n",
    "\n",
    "#observations = tf.placeholder(tf.float32, [None,D] , name=\"input_x\")\n",
    "W = tf.get_variable(\"W\", shape=[H, 3],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "score = tf.matmul(convFlat, W)\n",
    "probability = tf.nn.softmax(score)\n",
    "\n",
    "real_action = tf.placeholder(shape=[None, 3], dtype=tf.int32)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=real_action, \n",
    "                                                                     logits=score))\n",
    "tf.summary.scalar('loss', loss)\n",
    "train_step = tf.train.RMSPropOptimizer(0.001).minimize(loss)\n",
    "\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "MineRL package provides a human playing dataset for improving effiency of traning. At first, we are going to train our network by this dataset and use pretrained network for Reinforcement Learning. I assure it will reduce traing time tremendously. \n",
    "\n",
    "For more information about that dataset, see this [MineRL Dataset Docs](http://minerl.io/docs/tutorials/data_sampling.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c5575f0c4cb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mminerl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminerl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrestore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env_name' is not defined"
     ]
    }
   ],
   "source": [
    "import minerl\n",
    "data = minerl.data.make(env_name, data_path)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "restore = False\n",
    "with tf.Session() as sess:\n",
    "    rendering = False\n",
    "    sess.run(init)\n",
    "    saver = tf.train.Saver(max_to_keep=5)\n",
    "    train_writer = tf.summary.FileWriter('/home/kimbring2/MineRL/train_summary/' + env_name, sess.graph)\n",
    "    \n",
    "    if restore == True:\n",
    "        path = '/home/kimbring2/MineRL/model/' + env_name\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    \n",
    "    episode_count = 0\n",
    "    for current_state, action, reward, next_state, done in data.sarsd_iter(num_epochs=500, max_sequence_len=200):\n",
    "        #print(\"current_state['pov'].shape: \" + str(current_state['pov'].shape))\n",
    "        length = (current_state['pov'].shape)[0]\n",
    "\n",
    "        action_list = []\n",
    "        states_list = []\n",
    "        for i in range(0, length):\n",
    "            if (env_name == 'MineRLTreechop-v0'):\n",
    "                state_concat = current_state['pov'][i].astype(np.float32) / 255.0 - 0.5\n",
    "            elif (env_name == 'MineRLNavigateDense-v0'):\n",
    "                pov = current_state['pov'][i].astype(np.float32) / 255.0 - 0.5\n",
    "                compass = current_state['compassAngle'][i]\n",
    "                compass_channel = np.ones(shape=list(pov.shape[:-1]) + [1], dtype=np.float32) * compass\n",
    "                compass_channel /= 180.0\n",
    "        \n",
    "                state_concat = np.concatenate([pov, compass_channel], axis=-1)\n",
    "            \n",
    "            if (env_name == 'MineRLNavigateDense-v0'):\n",
    "                if (action['camera'][i][1] < 0):\n",
    "                    action_ = [1, 0, 0, 0]\n",
    "                elif (action['camera'][i][1] > 0):\n",
    "                    action_ = [0, 1, 0, 0]\n",
    "                else:\n",
    "                    if (action['jump'][i] == 0):\n",
    "                        action_ = [0, 0, 1, 0]\n",
    "                    else:\n",
    "                        action_ = [0, 0, 0, 1]\n",
    "            elif (env_name == 'MineRLTreechop-v0'):\n",
    "                if (action['camera'][i][1] < 0):\n",
    "                    action_ = [1, 0, 0, 0, 0, 0]\n",
    "                elif (action['camera'][i][1] > 0):\n",
    "                    action_ = [0, 1, 0, 0, 0, 0]\n",
    "                elif (action['camera'][i][0] < 0):\n",
    "                    action_ = [0, 0, 1, 0, 0, 0]\n",
    "                elif (action['camera'][i][0] > 0):\n",
    "                    action_ = [0, 0, 0, 1, 0, 0]\n",
    "                else:\n",
    "                    if ( (action['jump'][i] == 0) & (action['forward'][i] == 0) ):\n",
    "                        action_ = [0, 0, 0, 0, 1, 0]\n",
    "                    elif ( (action['jump'][i] == 1) & (action['forward'][i] == 1) ):\n",
    "                        action_ = [0, 0, 0, 0, 0, 1]\n",
    "            \n",
    "            states_list.append(state_concat)\n",
    "            action_list.append(action_)\n",
    "        \n",
    "        episode_count = episode_count + 1\n",
    "        \n",
    "        rnn_state = state_init_self\n",
    "        batch_rnn_state = rnn_state\n",
    "        #state_train = (np.zeros([1,H]), np.zeros([1,H]))\n",
    "        feed_dict = {state:np.stack(states_list, 0),\n",
    "                     real_action:np.stack(action_list, 0),\n",
    "                     state_in_self[0]:rnn_state[0],\n",
    "                     state_in_self[1]:rnn_state[1]\n",
    "                    }\n",
    "        \n",
    "        if episode_count % 100 == 0:\n",
    "            summary, _ = sess.run([merged, train_step], feed_dict=feed_dict)\n",
    "            train_writer.add_summary(summary, episode_count)\n",
    "\n",
    "        sess.run(train_step, feed_dict=feed_dict)\n",
    "        \n",
    "        if episode_count % 100 == 0:\n",
    "            model_path = '/home/kimbring2/MineRL/model/' + env_name\n",
    "            saver.save(sess, model_path + '/model-' + str(episode_count) + '.cptk')\n",
    "            print(\"Saved Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Agent and Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we run the neural network agent, and have it act in the CartPole environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport minerl\\nimport gym\\nenv = gym.make(\\'MineRLNavigateDense-v0\\')\\n\\nobs  = env.reset()\\ndone = False\\nnet_reward = 0\\n\\nwhile not done:\\n    action = env.action_space.noop()\\n\\n    action[\\'camera\\'] = [0, -10]\\n    action[\\'back\\'] = 0\\n    action[\\'forward\\'] = 1\\n    action[\\'jump\\'] = 1\\n    action[\\'attack\\'] = 1\\n\\n    obs, reward, done, info = env.step(\\n        action)\\n\\n    net_reward += reward\\n    print(\"Total reward: \", net_reward)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import minerl\n",
    "import gym\n",
    "env = gym.make('MineRLNavigateDense-v0')\n",
    "\n",
    "obs  = env.reset()\n",
    "done = False\n",
    "net_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.noop()\n",
    "\n",
    "    action['camera'] = [0, -10]\n",
    "    action['back'] = 0\n",
    "    action['forward'] = 1\n",
    "    action['jump'] = 1\n",
    "    action['attack'] = 1\n",
    "\n",
    "    obs, reward, done, info = env.step(\n",
    "        action)\n",
    "\n",
    "    net_reward += reward\n",
    "    print(\"Total reward: \", net_reward)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import minerl\n",
    "#data = minerl.data.make('MineRLNavigateDense-v0', '/home/kimbring2/MineRL/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import minerl\n",
    "import gym\n",
    "\n",
    "env = gym.make('MineRLNavigateDense-v0')\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n",
      "WARNING:tensorflow:From /home/kimbring2/minerl_env/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from /home/kimbring2/MineRL/model/model-60600.cptk\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "# Launch the graph\n",
    "    rendering = False\n",
    "    sess.run(init)\n",
    "    saver = tf.train.Saver(max_to_keep=5)\n",
    "    train_writer = tf.summary.FileWriter('/home/kimbring2/MineRL/train_summary', sess.graph)\n",
    "    \n",
    "    print('Loading Model...')\n",
    "    path = '/home/kimbring2/MineRL/model'\n",
    "    ckpt = tf.train.get_checkpoint_state(path)\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    \n",
    "    env.init()\n",
    "    obs = env.reset()\n",
    "    net_reward = 0\n",
    "    for i in range(0, 500000):\n",
    "        pov = obs['pov'].astype(np.float32) / 255.0 - 0.5\n",
    "        compass = obs['compassAngle']\n",
    "\n",
    "        compass_channel = np.ones(shape=list(pov.shape[:-1]) + [1], dtype=np.float32) * compass\n",
    "        compass_channel /= 180.0\n",
    "        \n",
    "        state_concat = np.concatenate([pov, compass_channel], axis=-1)\n",
    "        action_probability = sess.run(probability, feed_dict={state:[state_concat]})\n",
    "\n",
    "        action = env.action_space.noop()\n",
    "        if (np.argmax(action_probability) == 0):\n",
    "            action['camera'] = [0, -10]\n",
    "            action['jump'] = 0\n",
    "        elif (np.argmax(action_probability) == 1):\n",
    "            action['camera'] = [0, 10]\n",
    "            action['jump'] = 0\n",
    "        else:\n",
    "            action['camera'] = [0, 0]\n",
    "            action['jump'] = 1\n",
    "        \n",
    "        action['forward'] = 1\n",
    "        action['back'] = 0\n",
    "        action['left'] = 0\n",
    "        #action['jump'] = np.argmax(j)\n",
    "        action['right'] = 0\n",
    "        action['sprint'] = 1\n",
    "\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done == True:\n",
    "            break\n",
    "\n",
    "        net_reward += reward\n",
    "        print(\"Total reward: \", net_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the network not only does much better than random actions, but achieves the goal of 200 points per episode, thus solving the task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "minerl_env",
   "language": "python",
   "name": "minerl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
