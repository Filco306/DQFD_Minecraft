{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Imitation Learning in MineRL\n",
    "This tutorial contains a simple example of how to build a imitation-learning based agent that can solve the MineRLNavigateDense-v0 environment. For more information about that environment, see this [MineRL Docs](http://minerl.io/docs/environments/index.html#minerlnavigatedense-v0).\n",
    "\n",
    "For more Imitation Learning algorithms, like a Dagger in Tensorflow, see that Github repo, [Dagger](https://github.com/zsdonghao/Imitation-Learning-Dagger-Torcs).\n",
    "\n",
    "Parts of this tutorial are based on code by [Arthur Juliani](https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kimbring2/minerl_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/kimbring2/minerl_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/kimbring2/minerl_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/kimbring2/minerl_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/kimbring2/minerl_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/kimbring2/minerl_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "try:\n",
    "    xrange = xrange\n",
    "except:\n",
    "    xrange = range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the CartPole Environment\n",
    "If you don't already have the OpenAI gym installed, use  `pip install gym` to grab it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we try running the environment with random actions? How well do we do? (Hint: not so well.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the task is to achieve a reward of 200 per episode. For every step the agent keeps the pole in the air, the agent recieves a +1 reward. By randomly choosing actions, our reward for each episode is only a couple dozen. Let's make that better with RL!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up our Neural Network agent\n",
    "This time we will be using a Policy neural network that takes observations, passes them through a single hidden layer, and then produces a probability of choosing a left/right movement. To learn more about this network, see [Andrej Karpathy's blog on Policy Gradient networks](http://karpathy.github.io/2016/05/31/rl/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kimbring2/minerl_env/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/kimbring2/minerl_env/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "convFlat: Tensor(\"Flatten/flatten/Reshape:0\", shape=(?, 57600), dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-2-945474e8b59f>:30: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "H = 57600\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "state = tf.placeholder(shape=[None,64,64,4], dtype=tf.float32)\n",
    "conv1 = slim.conv2d( \\\n",
    "            inputs=state,num_outputs=16,kernel_size=[2,2],stride=[1,1],padding='VALID', \n",
    "                    biases_initializer=None,activation_fn=tf.nn.relu)\n",
    "conv2 = slim.conv2d( \\\n",
    "            inputs=conv1,num_outputs=16,kernel_size=[2,2],stride=[1,1],padding='VALID', \n",
    "                    biases_initializer=None,activation_fn=tf.nn.relu)\n",
    "conv3 = slim.conv2d( \\\n",
    "            inputs=conv2,num_outputs=16,kernel_size=[2,2],stride=[1,1],padding='VALID', \n",
    "                    biases_initializer=None,activation_fn=tf.nn.relu)\n",
    "conv4 = slim.conv2d( \\\n",
    "            inputs=conv3,num_outputs=16,kernel_size=[2,2],stride=[1,1],padding='VALID', \n",
    "                    biases_initializer=None,activation_fn=tf.nn.relu)\n",
    "\n",
    "convFlat = slim.flatten(conv4)\n",
    "print(\"convFlat: \" + str(convFlat))\n",
    "\n",
    "#observations = tf.placeholder(tf.float32, [None,D] , name=\"input_x\")\n",
    "W = tf.get_variable(\"W\", shape=[H, 3],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "score = tf.matmul(convFlat, W)\n",
    "probability = tf.nn.softmax(score)\n",
    "\n",
    "real_action = tf.placeholder(shape=[None, 3], dtype=tf.int32)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=real_action, \n",
    "                                                                     logits=score))\n",
    "tf.summary.scalar('loss', loss)\n",
    "train_step = tf.train.RMSPropOptimizer(0.001).minimize(loss)\n",
    "\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantage function\n",
    "This function allows us to weigh the rewards our agent recieves. In the context of the Cart-Pole task, we want actions that kept the pole in the air a long time to have a large reward, and actions that contributed to the pole falling to have a decreased or negative reward. We do this by weighing the rewards from the end of the episode, with actions at the end being seen as negative, since they likely contributed to the pole falling, and the episode ending. Likewise, early actions are seen as more positive, since they weren't responsible for the pole falling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Agent and Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we run the neural network agent, and have it act in the CartPole environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport minerl\\nimport gym\\nenv = gym.make(\\'MineRLNavigateDense-v0\\')\\n\\nobs  = env.reset()\\ndone = False\\nnet_reward = 0\\n\\nwhile not done:\\n    action = env.action_space.noop()\\n\\n    action[\\'camera\\'] = [0, -10]\\n    action[\\'back\\'] = 0\\n    action[\\'forward\\'] = 1\\n    action[\\'jump\\'] = 1\\n    action[\\'attack\\'] = 1\\n\\n    obs, reward, done, info = env.step(\\n        action)\\n\\n    net_reward += reward\\n    print(\"Total reward: \", net_reward)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import minerl\n",
    "import gym\n",
    "env = gym.make('MineRLNavigateDense-v0')\n",
    "\n",
    "obs  = env.reset()\n",
    "done = False\n",
    "net_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.noop()\n",
    "\n",
    "    action['camera'] = [0, -10]\n",
    "    action['back'] = 0\n",
    "    action['forward'] = 1\n",
    "    action['jump'] = 1\n",
    "    action['attack'] = 1\n",
    "\n",
    "    obs, reward, done, info = env.step(\n",
    "        action)\n",
    "\n",
    "    net_reward += reward\n",
    "    print(\"Total reward: \", net_reward)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import minerl\n",
    "#data = minerl.data.make('MineRLNavigateDense-v0', '/home/kimbring2/MineRL/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import minerl\n",
    "import gym\n",
    "\n",
    "env = gym.make('MineRLNavigateDense-v0')\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n",
      "WARNING:tensorflow:From /home/kimbring2/minerl_env/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from /home/kimbring2/MineRL/model/model-60600.cptk\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "# Launch the graph\n",
    "    rendering = False\n",
    "    sess.run(init)\n",
    "    saver = tf.train.Saver(max_to_keep=5)\n",
    "    train_writer = tf.summary.FileWriter('/home/kimbring2/MineRL/train_summary', sess.graph)\n",
    "    \n",
    "    print('Loading Model...')\n",
    "    path = '/home/kimbring2/MineRL/model'\n",
    "    ckpt = tf.train.get_checkpoint_state(path)\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    \n",
    "    env.init()\n",
    "    obs = env.reset()\n",
    "    net_reward = 0\n",
    "    for i in range(0, 500000):\n",
    "        pov = obs['pov'].astype(np.float32) / 255.0 - 0.5\n",
    "        compass = obs['compassAngle']\n",
    "\n",
    "        compass_channel = np.ones(shape=list(pov.shape[:-1]) + [1], dtype=np.float32) * compass\n",
    "        compass_channel /= 180.0\n",
    "        \n",
    "        state_concat = np.concatenate([pov, compass_channel], axis=-1)\n",
    "        action_probability = sess.run(probability, feed_dict={state:[state_concat]})\n",
    "\n",
    "        action = env.action_space.noop()\n",
    "        if (np.argmax(action_probability) == 0):\n",
    "            action['camera'] = [0, -10]\n",
    "            action['jump'] = 0\n",
    "        elif (np.argmax(action_probability) == 1):\n",
    "            action['camera'] = [0, 10]\n",
    "            action['jump'] = 0\n",
    "        else:\n",
    "            action['camera'] = [0, 0]\n",
    "            action['jump'] = 1\n",
    "        \n",
    "        action['forward'] = 1\n",
    "        action['back'] = 0\n",
    "        action['left'] = 0\n",
    "        #action['jump'] = np.argmax(j)\n",
    "        action['right'] = 0\n",
    "        action['sprint'] = 1\n",
    "\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done == True:\n",
    "            break\n",
    "\n",
    "        net_reward += reward\n",
    "        print(\"Total reward: \", net_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the network not only does much better than random actions, but achieves the goal of 200 points per episode, thus solving the task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "minerl_env",
   "language": "python",
   "name": "minerl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
