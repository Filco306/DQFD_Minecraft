{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Reinforcement Learning in MineRL\n",
    "This tutorial contains a simple example of how to build a imitation-learning based agent that can solve the MineRLNavigateDense-v0 environment. For more information about that environment, see this [MineRL Docs](http://minerl.io/docs/environments/index.html#minerlnavigatedense-v0).\n",
    "\n",
    "For more Imitation Learning algorithms, like a Dagger in Tensorflow, see that Github repo, [Dagger](https://github.com/zsdonghao/Imitation-Learning-Dagger-Torcs).\n",
    "\n",
    "Parts of this tutorial are based on code by [Arthur Juliani](https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kimbring2/minerl_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/kimbring2/minerl_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/kimbring2/minerl_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/kimbring2/minerl_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/kimbring2/minerl_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/kimbring2/minerl_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "try:\n",
    "    xrange = xrange\n",
    "except:\n",
    "    xrange = range\n",
    "    \n",
    "#env_name = 'MineRLNavigateDense-v0'\n",
    "env_name = 'MineRLTreechop-v0'\n",
    "data_path = '/media/kimbring2/6224AA7924AA5039/minerl_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "\n",
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up our Neural Network agent\n",
    "This time we will be using a Convolutional Neural Network that takes observations, passes them through a single hidden layer, and then produces a probability of choosing a Jump and Camera movement. To learn more about this network, see [Convolutional Neural Networks for Visual Recognition Course](http://cs231n.stanford.edu/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kimbring2/minerl_env/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/kimbring2/minerl_env/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /home/kimbring2/minerl_env/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kimbring2/minerl_env/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "H = 1024\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "if (env_name == 'MineRLTreechop-v0'):\n",
    "    state = tf.placeholder(shape=[None,64,64,3], dtype=tf.float32)\n",
    "elif (env_name == 'MineRLNavigateDense-v0'):\n",
    "    state = tf.placeholder(shape=[None,64,64,4], dtype=tf.float32)\n",
    "\n",
    "conv1 = slim.conv2d( \\\n",
    "            inputs=state,num_outputs=32,kernel_size=[8,8],stride=[4,4],padding='VALID', \n",
    "                    biases_initializer=None,activation_fn=tf.nn.relu)\n",
    "conv2 = slim.conv2d( \\\n",
    "            inputs=conv1,num_outputs=64,kernel_size=[4,4],stride=[2,2],padding='VALID', \n",
    "                    biases_initializer=None,activation_fn=tf.nn.relu)\n",
    "conv3 = slim.conv2d( \\\n",
    "            inputs=conv2,num_outputs=64,kernel_size=[3,3],stride=[1,1],padding='VALID', \n",
    "                    biases_initializer=None,activation_fn=tf.nn.relu)\n",
    "\n",
    "convFlat = slim.flatten(conv3)\n",
    "#print(\"convFlat: \" + str(convFlat))\n",
    "\n",
    "if (env_name == 'MineRLTreechop-v0'):\n",
    "    W = tf.get_variable(\"W\", shape=[H,6],\n",
    "               initializer=tf.contrib.layers.xavier_initializer())\n",
    "    score = tf.matmul(convFlat, W)\n",
    "    probability = tf.nn.softmax(score)\n",
    "    #real_action = tf.placeholder(shape=[None,6], dtype=tf.int32)\n",
    "elif (env_name == 'MineRLNavigateDense-v0'):\n",
    "    W = tf.get_variable(\"W\", shape=[H,4],\n",
    "               initializer=tf.contrib.layers.xavier_initializer())\n",
    "    score = tf.matmul(convFlat, W)\n",
    "    probability = tf.nn.softmax(score)\n",
    "    #real_action = tf.placeholder(shape=[None,4], dtype=tf.int32)\n",
    "\n",
    "reward_holder = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "action_holder = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        \n",
    "indexes = tf.range(0, tf.shape(probability)[0]) * tf.shape(probability)[1] + action_holder\n",
    "responsible_outputs = tf.gather(tf.reshape(probability, [-1]), indexes)\n",
    "\n",
    "loss = -tf.reduce_mean(tf.log(responsible_outputs) * reward_holder)\n",
    "    \n",
    "#loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=real_action, \n",
    "#                                                                     logits=score))\n",
    "#tf.summary.scalar('loss', loss)\n",
    "\n",
    "tvars = tf.trainable_variables()\n",
    "gradient_holders = []\n",
    "for idx, var in enumerate(tvars):\n",
    "    placeholder = tf.placeholder(tf.float32, name=str(idx) + '_holder')\n",
    "    gradient_holders.append(placeholder)\n",
    "        \n",
    "gradients = tf.gradients(loss, tvars)\n",
    "        \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "update_batch = optimizer.apply_gradients(zip(gradient_holders,tvars))\n",
    "\n",
    "#train_step = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "#merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    (noop) : 'MineRLNavigateDense-v0'\\n    {'jump': 0, 'camera': [0, 0]} = 3\\n    {'jump': 1, 'camera': [0, 0]} = 4\\n    {'jump': 0, 'camera': [0, -10]} = 1\\n    {'jump': 0, 'camera': [0, 10]} = 2\\n            \\n    (noop) : 'MineRLTreechop-v0'\\n    {'forward': 0, 'jump': 0, 'camera': [0, 0]} = 5\\n    {'forward': 1, 'jump': 1, 'camera': [0, 0]} = 6\\n    {'forward': 1, 'jump': 0, 'camera': [0, -10]} = 1\\n    {'forward': 1, 'jump': 0, 'camera': [0, 10]} = 2\\n    {'forward': 1, 'jump': 0, 'camera': [-10, 0]} = 3\\n    {'forward': 1, 'jump': 0, 'camera': [10, 0]} = 4\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    (noop) : 'MineRLNavigateDense-v0'\n",
    "    {'jump': 0, 'camera': [0, 0]} = 3\n",
    "    {'jump': 1, 'camera': [0, 0]} = 4\n",
    "    {'jump': 0, 'camera': [0, -10]} = 1\n",
    "    {'jump': 0, 'camera': [0, 10]} = 2\n",
    "            \n",
    "    (noop) : 'MineRLTreechop-v0'\n",
    "    {'forward': 0, 'jump': 0, 'camera': [0, 0]} = 5\n",
    "    {'forward': 1, 'jump': 1, 'camera': [0, 0]} = 6\n",
    "    {'forward': 1, 'jump': 0, 'camera': [0, -10]} = 1\n",
    "    {'forward': 1, 'jump': 0, 'camera': [0, 10]} = 2\n",
    "    {'forward': 1, 'jump': 0, 'camera': [-10, 0]} = 3\n",
    "    {'forward': 1, 'jump': 0, 'camera': [10, 0]} = 4\n",
    "''' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n",
      "WARNING:tensorflow:From /home/kimbring2/minerl_env/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from /home/kimbring2/MineRL_Git/model/MineRLTreechop-v0/model-114300.cptk\n",
      "Total reward:  17.0\n",
      "e:  0.05\n",
      "Saved Model\n",
      "\n",
      "Total reward:  12.0\n",
      "e:  0.04995\n",
      "Saved Model\n",
      "\n",
      "Total reward:  22.0\n",
      "e:  0.04990005\n",
      "Saved Model\n",
      "\n",
      "Total reward:  13.0\n",
      "e:  0.04985014995\n",
      "Saved Model\n",
      "\n",
      "Total reward:  9.0\n",
      "e:  0.04980029980005\n",
      "Saved Model\n",
      "\n",
      "Total reward:  11.0\n",
      "e:  0.04975049950024995\n",
      "WARNING:tensorflow:From /home/kimbring2/minerl_env/lib/python3.6/site-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Saved Model\n",
      "\n",
      "Total reward:  15.0\n",
      "e:  0.0497007490007497\n",
      "Saved Model\n",
      "\n",
      "Total reward:  0.0\n",
      "e:  0.04965104825174895\n",
      "Saved Model\n",
      "\n",
      "Total reward:  13.0\n",
      "e:  0.049601397203497204\n",
      "Saved Model\n",
      "\n",
      "Total reward:  16.0\n",
      "e:  0.04955179580629371\n",
      "Saved Model\n",
      "\n",
      "Total reward:  6.0\n",
      "e:  0.04950224401048742\n",
      "Saved Model\n",
      "\n",
      "Total reward:  3.0\n",
      "e:  0.04945274176647693\n",
      "Saved Model\n",
      "\n",
      "Total reward:  18.0\n",
      "e:  0.04940328902471045\n",
      "Saved Model\n",
      "\n",
      "Total reward:  14.0\n",
      "e:  0.04935388573568574\n",
      "Saved Model\n",
      "\n",
      "Total reward:  31.0\n",
      "e:  0.04930453184995005\n",
      "Saved Model\n",
      "\n",
      "Total reward:  0.0\n",
      "e:  0.0492552273181001\n",
      "Saved Model\n",
      "\n",
      "Total reward:  8.0\n",
      "e:  0.049205972090782\n",
      "Saved Model\n",
      "\n",
      "Total reward:  17.0\n",
      "e:  0.04915676611869122\n",
      "Saved Model\n",
      "\n",
      "Total reward:  0.0\n",
      "e:  0.04910760935257253\n",
      "Saved Model\n",
      "\n",
      "Total reward:  12.0\n",
      "e:  0.049058501743219955\n",
      "Saved Model\n",
      "\n",
      "Total reward:  12.0\n",
      "e:  0.049009443241476734\n",
      "Saved Model\n",
      "\n",
      "Total reward:  6.0\n",
      "e:  0.04896043379823526\n",
      "Saved Model\n",
      "\n",
      "Total reward:  0.0\n",
      "e:  0.048911473364437026\n",
      "Saved Model\n",
      "\n",
      "Total reward:  12.0\n",
      "e:  0.04886256189107259\n",
      "Saved Model\n",
      "\n",
      "Total reward:  4.0\n",
      "e:  0.04881369932918152\n",
      "Saved Model\n",
      "\n",
      "Total reward:  4.0\n",
      "e:  0.04876488562985234\n",
      "Saved Model\n",
      "\n",
      "Total reward:  8.0\n",
      "e:  0.048716120744222484\n",
      "Saved Model\n",
      "\n",
      "Total reward:  11.0\n",
      "e:  0.04866740462347826\n",
      "Saved Model\n",
      "\n",
      "Total reward:  26.0\n",
      "e:  0.048618737218854784\n",
      "Saved Model\n",
      "\n",
      "Total reward:  5.0\n",
      "e:  0.04857011848163593\n",
      "Saved Model\n",
      "\n",
      "Total reward:  11.0\n",
      "e:  0.04852154836315429\n",
      "Saved Model\n",
      "\n",
      "Total reward:  6.0\n",
      "e:  0.04847302681479113\n",
      "Saved Model\n",
      "\n",
      "Total reward:  4.0\n",
      "e:  0.04842455378797634\n",
      "Saved Model\n",
      "\n",
      "Total reward:  19.0\n",
      "e:  0.04837612923418836\n",
      "Saved Model\n",
      "\n",
      "Total reward:  11.0\n",
      "e:  0.04832775310495417\n",
      "Saved Model\n",
      "\n",
      "Total reward:  19.0\n",
      "e:  0.04827942535184922\n",
      "Saved Model\n",
      "\n",
      "Total reward:  12.0\n",
      "e:  0.04823114592649737\n",
      "Saved Model\n",
      "\n",
      "Total reward:  2.0\n",
      "e:  0.04818291478057087\n",
      "Saved Model\n",
      "\n",
      "Total reward:  16.0\n",
      "e:  0.0481347318657903\n",
      "Saved Model\n",
      "\n",
      "Total reward:  11.0\n",
      "e:  0.04808659713392451\n",
      "Saved Model\n",
      "\n",
      "Total reward:  7.0\n",
      "e:  0.04803851053679059\n",
      "Saved Model\n",
      "\n",
      "Total reward:  9.0\n",
      "e:  0.0479904720262538\n",
      "Saved Model\n",
      "\n",
      "Total reward:  10.0\n",
      "e:  0.04794248155422754\n",
      "Saved Model\n",
      "\n",
      "Total reward:  16.0\n",
      "e:  0.04789453907267331\n",
      "Saved Model\n",
      "\n",
      "Total reward:  15.0\n",
      "e:  0.04784664453360064\n",
      "Saved Model\n",
      "\n",
      "Total reward:  8.0\n",
      "e:  0.04779879788906704\n",
      "Saved Model\n",
      "\n",
      "Total reward:  22.0\n",
      "e:  0.04775099909117797\n",
      "Saved Model\n",
      "\n",
      "Total reward:  10.0\n",
      "e:  0.04770324809208679\n",
      "Saved Model\n",
      "\n",
      "Total reward:  5.0\n",
      "e:  0.04765554484399471\n",
      "Saved Model\n",
      "\n",
      "Total reward:  3.0\n",
      "e:  0.04760788929915071\n",
      "Saved Model\n",
      "\n",
      "Total reward:  17.0\n",
      "e:  0.04756028140985156\n",
      "Saved Model\n",
      "\n",
      "Total reward:  5.0\n",
      "e:  0.04751272112844171\n",
      "Saved Model\n",
      "\n",
      "Total reward:  8.0\n",
      "e:  0.04746520840731327\n",
      "Saved Model\n",
      "\n",
      "Total reward:  15.0\n",
      "e:  0.04741774319890595\n",
      "Saved Model\n",
      "\n",
      "Total reward:  12.0\n",
      "e:  0.047370325455707046\n",
      "Saved Model\n",
      "\n",
      "Total reward:  3.0\n",
      "e:  0.04732295513025134\n",
      "Saved Model\n",
      "\n",
      "Total reward:  4.0\n",
      "e:  0.04727563217512109\n",
      "Saved Model\n",
      "\n",
      "Total reward:  2.0\n",
      "e:  0.047228356542945965\n",
      "Saved Model\n",
      "\n",
      "Total reward:  7.0\n",
      "e:  0.04718112818640302\n",
      "Saved Model\n",
      "\n",
      "Total reward:  8.0\n",
      "e:  0.047133947058216615\n",
      "Saved Model\n",
      "\n",
      "Total reward:  5.0\n",
      "e:  0.0470868131111584\n",
      "Saved Model\n",
      "\n",
      "Total reward:  18.0\n",
      "e:  0.047039726298047244\n",
      "Saved Model\n",
      "\n",
      "Total reward:  17.0\n",
      "e:  0.046992686571749195\n",
      "Saved Model\n",
      "\n",
      "Total reward:  5.0\n",
      "e:  0.04694569388517745\n",
      "Saved Model\n",
      "\n",
      "Total reward:  4.0\n",
      "e:  0.04689874819129227\n",
      "Saved Model\n",
      "\n",
      "Total reward:  18.0\n",
      "e:  0.04685184944310098\n",
      "Saved Model\n",
      "\n",
      "Total reward:  4.0\n",
      "e:  0.04680499759365788\n",
      "Saved Model\n",
      "\n",
      "Total reward:  27.0\n",
      "e:  0.04675819259606422\n",
      "Saved Model\n",
      "\n",
      "Total reward:  8.0\n",
      "e:  0.04671143440346816\n",
      "Saved Model\n",
      "\n",
      "Total reward:  17.0\n",
      "e:  0.04666472296906469\n",
      "Saved Model\n",
      "\n",
      "Total reward:  13.0\n",
      "e:  0.04661805824609563\n",
      "Saved Model\n",
      "\n",
      "Total reward:  33.0\n",
      "e:  0.04657144018784953\n",
      "Saved Model\n",
      "\n",
      "Total reward:  1.0\n",
      "e:  0.046524868747661687\n",
      "Saved Model\n",
      "\n",
      "Total reward:  10.0\n",
      "e:  0.04647834387891402\n",
      "Saved Model\n",
      "\n",
      "Total reward:  21.0\n",
      "e:  0.04643186553503511\n",
      "Saved Model\n",
      "\n",
      "Total reward:  10.0\n",
      "e:  0.04638543366950008\n",
      "Saved Model\n",
      "\n",
      "Total reward:  11.0\n",
      "e:  0.04633904823583058\n",
      "Saved Model\n",
      "\n",
      "Total reward:  21.0\n",
      "e:  0.04629270918759475\n",
      "Saved Model\n",
      "\n",
      "Total reward:  26.0\n",
      "e:  0.04624641647840715\n",
      "Saved Model\n",
      "\n",
      "Total reward:  0.0\n",
      "e:  0.046200170061928746\n",
      "Saved Model\n",
      "\n",
      "Total reward:  2.0\n",
      "e:  0.04615396989186682\n",
      "Saved Model\n",
      "\n",
      "Total reward:  8.0\n",
      "e:  0.04610781592197495\n",
      "Saved Model\n",
      "\n",
      "Total reward:  4.0\n",
      "e:  0.04606170810605298\n",
      "Saved Model\n",
      "\n",
      "Total reward:  12.0\n",
      "e:  0.046015646397946926\n",
      "Saved Model\n",
      "\n",
      "Total reward:  0.0\n",
      "e:  0.04596963075154898\n",
      "Saved Model\n",
      "\n",
      "Total reward:  19.0\n",
      "e:  0.04592366112079743\n",
      "Saved Model\n",
      "\n",
      "Total reward:  7.0\n",
      "e:  0.04587773745967663\n",
      "Saved Model\n",
      "\n",
      "Total reward:  9.0\n",
      "e:  0.045831859722216955\n",
      "Saved Model\n",
      "\n",
      "Total reward:  20.0\n",
      "e:  0.045786027862494735\n",
      "Saved Model\n",
      "\n",
      "Total reward:  13.0\n",
      "e:  0.04574024183463224\n",
      "Saved Model\n",
      "\n",
      "Total reward:  20.0\n",
      "e:  0.04569450159279761\n",
      "Saved Model\n",
      "\n",
      "Total reward:  11.0\n",
      "e:  0.04564880709120481\n",
      "Saved Model\n",
      "\n",
      "Total reward:  8.0\n",
      "e:  0.045603158284113605\n",
      "Saved Model\n",
      "\n",
      "Total reward:  8.0\n",
      "e:  0.04555755512582949\n",
      "Saved Model\n",
      "\n",
      "Total reward:  28.0\n",
      "e:  0.04551199757070366\n",
      "Saved Model\n",
      "\n",
      "Total reward:  24.0\n",
      "e:  0.04546648557313296\n",
      "Saved Model\n",
      "\n",
      "Total reward:  9.0\n",
      "e:  0.04542101908755983\n",
      "Saved Model\n",
      "\n",
      "Total reward:  15.0\n",
      "e:  0.045375598068472266\n",
      "Saved Model\n",
      "\n",
      "Total reward:  13.0\n",
      "e:  0.045330222470403796\n",
      "Saved Model\n",
      "\n",
      "Total reward:  9.0\n",
      "e:  0.04528489224793339\n",
      "Saved Model\n",
      "\n",
      "Total reward:  11.0\n",
      "e:  0.045239607355685454\n",
      "Saved Model\n",
      "\n",
      "Total reward:  16.0\n",
      "e:  0.04519436774832977\n",
      "Saved Model\n",
      "\n",
      "Total reward:  15.0\n",
      "e:  0.04514917338058144\n",
      "Saved Model\n",
      "\n",
      "Total reward:  12.0\n",
      "e:  0.04510402420720086\n",
      "Saved Model\n",
      "\n",
      "Total reward:  13.0\n",
      "e:  0.04505892018299366\n",
      "Saved Model\n",
      "\n",
      "Total reward:  1.0\n",
      "e:  0.045013861262810664\n",
      "Saved Model\n",
      "\n",
      "Total reward:  12.0\n",
      "e:  0.04496884740154786\n",
      "Saved Model\n",
      "\n",
      "Total reward:  4.0\n",
      "e:  0.044923878554146306\n",
      "Saved Model\n",
      "\n",
      "Total reward:  23.0\n",
      "e:  0.04487895467559216\n",
      "Saved Model\n",
      "\n",
      "Total reward:  25.0\n",
      "e:  0.044834075720916564\n",
      "Saved Model\n",
      "\n",
      "Total reward:  0.0\n",
      "e:  0.04478924164519565\n",
      "Saved Model\n",
      "\n",
      "Total reward:  6.0\n",
      "e:  0.044744452403550454\n",
      "Saved Model\n",
      "\n",
      "Total reward:  17.0\n",
      "e:  0.0446997079511469\n",
      "Saved Model\n",
      "\n",
      "Total reward:  3.0\n",
      "e:  0.04465500824319576\n",
      "Saved Model\n",
      "\n",
      "Total reward:  0.0\n",
      "e:  0.04461035323495256\n",
      "Saved Model\n",
      "\n",
      "Total reward:  24.0\n",
      "e:  0.04456574288171761\n",
      "Saved Model\n",
      "\n",
      "Total reward:  11.0\n",
      "e:  0.04452117713883589\n",
      "Saved Model\n",
      "\n",
      "Total reward:  27.0\n",
      "e:  0.04447665596169705\n",
      "Saved Model\n",
      "\n",
      "Total reward:  24.0\n",
      "e:  0.044432179305735356\n",
      "Saved Model\n",
      "\n",
      "Total reward:  15.0\n",
      "e:  0.04438774712642962\n",
      "Saved Model\n",
      "\n",
      "Total reward:  11.0\n",
      "e:  0.04434335937930319\n",
      "Saved Model\n",
      "\n",
      "Total reward:  18.0\n",
      "e:  0.044299016019923886\n",
      "Saved Model\n",
      "\n",
      "Total reward:  3.0\n",
      "e:  0.04425471700390396\n",
      "Saved Model\n",
      "\n",
      "Total reward:  31.0\n",
      "e:  0.04421046228690006\n",
      "Saved Model\n",
      "\n",
      "Total reward:  0.0\n",
      "e:  0.04416625182461316\n",
      "Saved Model\n",
      "\n",
      "Total reward:  7.0\n",
      "e:  0.044122085572788544\n",
      "Saved Model\n",
      "\n",
      "Total reward:  20.0\n",
      "e:  0.04407796348721576\n",
      "Saved Model\n",
      "\n",
      "Total reward:  14.0\n",
      "e:  0.04403388552372854\n",
      "Saved Model\n",
      "\n",
      "Total reward:  20.0\n",
      "e:  0.04398985163820481\n",
      "Saved Model\n",
      "\n",
      "Total reward:  9.0\n",
      "e:  0.043945861786566606\n",
      "Saved Model\n",
      "\n",
      "Total reward:  6.0\n",
      "e:  0.04390191592478004\n",
      "Saved Model\n",
      "\n",
      "Total reward:  27.0\n",
      "e:  0.04385801400885526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n",
      "\n",
      "Total reward:  23.0\n",
      "e:  0.0438141559948464\n",
      "Saved Model\n",
      "\n",
      "Total reward:  18.0\n",
      "e:  0.043770341838851555\n",
      "Saved Model\n",
      "\n",
      "Total reward:  2.0\n",
      "e:  0.0437265714970127\n",
      "Saved Model\n",
      "\n",
      "Total reward:  6.0\n",
      "e:  0.04368284492551569\n",
      "Saved Model\n",
      "\n",
      "Total reward:  0.0\n",
      "e:  0.043639162080590176\n",
      "Saved Model\n",
      "\n",
      "Total reward:  14.0\n",
      "e:  0.043595522918509585\n",
      "Saved Model\n",
      "\n",
      "Total reward:  11.0\n",
      "e:  0.04355192739559108\n",
      "Saved Model\n",
      "\n",
      "Total reward:  18.0\n",
      "e:  0.043508375468195484\n",
      "Saved Model\n",
      "\n",
      "Total reward:  16.0\n",
      "e:  0.04346486709272729\n",
      "Saved Model\n",
      "\n",
      "Total reward:  3.0\n",
      "e:  0.043421402225634564\n",
      "Saved Model\n",
      "\n",
      "Total reward:  4.0\n",
      "e:  0.04337798082340893\n",
      "Saved Model\n",
      "\n",
      "Total reward:  13.0\n",
      "e:  0.04333460284258552\n",
      "Saved Model\n",
      "\n",
      "Total reward:  10.0\n",
      "e:  0.043291268239742935\n",
      "Saved Model\n",
      "\n",
      "Total reward:  7.0\n",
      "e:  0.04324797697150319\n",
      "Saved Model\n",
      "\n",
      "Total reward:  13.0\n",
      "e:  0.04320472899453169\n",
      "Saved Model\n",
      "\n",
      "Total reward:  6.0\n",
      "e:  0.04316152426553716\n",
      "Saved Model\n",
      "\n",
      "Total reward:  19.0\n",
      "e:  0.04311836274127162\n",
      "Saved Model\n",
      "\n",
      "Total reward:  18.0\n",
      "e:  0.04307524437853035\n",
      "Saved Model\n",
      "\n",
      "Total reward:  11.0\n",
      "e:  0.043032169134151824\n",
      "Saved Model\n",
      "\n",
      "Total reward:  2.0\n",
      "e:  0.042989136965017674\n",
      "Saved Model\n",
      "\n",
      "Total reward:  4.0\n",
      "e:  0.04294614782805266\n",
      "Saved Model\n",
      "\n",
      "Total reward:  12.0\n",
      "e:  0.04290320168022461\n",
      "Saved Model\n",
      "\n",
      "Total reward:  4.0\n",
      "e:  0.04286029847854438\n",
      "Saved Model\n",
      "\n",
      "Total reward:  13.0\n",
      "e:  0.04281743818006584\n",
      "Saved Model\n",
      "\n",
      "Total reward:  10.0\n",
      "e:  0.04277462074188577\n",
      "Saved Model\n",
      "\n",
      "Total reward:  13.0\n",
      "e:  0.042731846121143884\n",
      "Saved Model\n",
      "\n",
      "Total reward:  3.0\n",
      "e:  0.04268911427502274\n",
      "Saved Model\n",
      "\n",
      "Total reward:  15.0\n",
      "e:  0.042646425160747715\n",
      "Saved Model\n",
      "\n",
      "Total reward:  24.0\n",
      "e:  0.04260377873558697\n",
      "Saved Model\n",
      "\n",
      "Total reward:  21.0\n",
      "e:  0.04256117495685138\n",
      "Saved Model\n",
      "\n",
      "Total reward:  10.0\n",
      "e:  0.04251861378189453\n",
      "Saved Model\n",
      "\n",
      "Total reward:  9.0\n",
      "e:  0.04247609516811264\n",
      "Saved Model\n",
      "\n",
      "Total reward:  1.0\n",
      "e:  0.042433619072944524\n",
      "Saved Model\n",
      "\n",
      "Total reward:  23.0\n",
      "e:  0.04239118545387158\n",
      "Saved Model\n",
      "\n",
      "Total reward:  3.0\n",
      "e:  0.04234879426841771\n",
      "Saved Model\n",
      "\n",
      "Total reward:  16.0\n",
      "e:  0.04230644547414929\n",
      "Saved Model\n",
      "\n",
      "Total reward:  2.0\n",
      "e:  0.04226413902867514\n",
      "Saved Model\n",
      "\n",
      "Total reward:  20.0\n",
      "e:  0.04222187488964646\n",
      "Saved Model\n",
      "\n",
      "Total reward:  31.0\n",
      "e:  0.04217965301475681\n",
      "Saved Model\n",
      "\n",
      "Total reward:  12.0\n",
      "e:  0.042137473361742055\n",
      "Saved Model\n",
      "\n",
      "Total reward:  15.0\n",
      "e:  0.04209533588838031\n",
      "Saved Model\n",
      "\n",
      "Total reward:  4.0\n",
      "e:  0.04205324055249193\n",
      "Saved Model\n",
      "\n",
      "Total reward:  2.0\n",
      "e:  0.04201118731193944\n",
      "Saved Model\n",
      "\n",
      "Total reward:  3.0\n",
      "e:  0.0419691761246275\n",
      "Saved Model\n",
      "\n",
      "Total reward:  10.0\n",
      "e:  0.041927206948502875\n",
      "Saved Model\n",
      "\n",
      "Total reward:  10.0\n",
      "e:  0.04188527974155437\n",
      "Saved Model\n",
      "\n",
      "Total reward:  14.0\n",
      "e:  0.041843394461812816\n",
      "Saved Model\n",
      "\n",
      "Total reward:  16.0\n",
      "e:  0.041801551067351006\n",
      "Saved Model\n",
      "\n",
      "Total reward:  14.0\n",
      "e:  0.041759749516283654\n",
      "Saved Model\n",
      "\n",
      "Total reward:  15.0\n",
      "e:  0.04171798976676737\n",
      "Saved Model\n",
      "\n",
      "Total reward:  6.0\n",
      "e:  0.041676271777000604\n",
      "Saved Model\n",
      "\n",
      "Total reward:  5.0\n",
      "e:  0.0416345955052236\n",
      "Saved Model\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-fcfa29f372b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mnet_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mep_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/minerl_env/lib/python3.6/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/minerl_env/lib/python3.6/site-packages/minerl/env/core.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    410\u001b[0m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_up\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0;31m# We don't force the same seed every episode, you gotta send it yourself queen.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/minerl_env/lib/python3.6/site-packages/minerl/env/comms.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mPyro4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyroError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"An error occurred contacting the instance manager. Is it started!?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/minerl_env/lib/python3.6/site-packages/minerl/env/core.py\u001b[0m in \u001b[0;36m_start_up\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_peek_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Failed to reset (socket error), trying again!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/minerl_env/lib/python3.6/site-packages/minerl/env/core.py\u001b[0m in \u001b[0;36m_peek_obs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mpeek_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"<Peek/>\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0mcomms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeek_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient_socket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient_socket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/minerl_env/lib/python3.6/site-packages/minerl/env/comms.py\u001b[0m in \u001b[0;36mrecv_message\u001b[0;34m(sock)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrecv_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mlengthbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecvall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlengthbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/minerl_env/lib/python3.6/site-packages/minerl/env/comms.py\u001b[0m in \u001b[0;36mrecvall\u001b[0;34m(sock, count)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mnewbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnewbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import minerl\n",
    "import gym\n",
    "import os\n",
    "import random\n",
    "from env_wrappers import ContinuingTimeLimitMonitor\n",
    "\n",
    "env = gym.make(env_name)\n",
    "#env = ContinuingTimeLimitMonitor(env, os.path.join('/home/kimbring2/MineRL/', 'monitor'), mode='evaluation', \n",
    "#                                 video_callable=lambda episode_id: True)\n",
    "\n",
    "e = 0.05\n",
    "update_frequency = 100\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "# Launch the graph\n",
    "    sess.run(init)\n",
    "    saver = tf.train.Saver(max_to_keep=5)\n",
    "    \n",
    "    print('Loading Model...') \n",
    "    path = '/home/kimbring2/MineRL_Git/model/' + env_name\n",
    "    ckpt = tf.train.get_checkpoint_state(path)\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    \n",
    "    for i in range(5000):\n",
    "        total_reward = []\n",
    "        total_length = []\n",
    "    \n",
    "        gradBuffer = sess.run(tf.trainable_variables())\n",
    "        for ix, grad in enumerate(gradBuffer):\n",
    "            gradBuffer[ix] = grad * 0\n",
    "    \n",
    "        env.init()\n",
    "        obs = env.reset()\n",
    "        net_reward = 0\n",
    "        ep_history = []\n",
    "        while True:\n",
    "            if (env_name == 'MineRLTreechop-v0'):\n",
    "                state_concat = obs['pov'].astype(np.float32) / 255.0 - 0.5\n",
    "            elif (env_name == 'MineRLNavigateDense-v0'):\n",
    "                pov = obs['pov'].astype(np.float32) / 255.0 - 0.5\n",
    "                compass = obs['compassAngle']\n",
    "                compass_channel = np.ones(shape=list(pov.shape[:-1]) + [1], dtype=np.float32) * compass\n",
    "                compass_channel /= 180.0\n",
    "                state_concat = np.concatenate([pov, compass_channel], axis=-1)\n",
    "            \n",
    "            action_probability = sess.run(probability, feed_dict={state:[state_concat]})\n",
    "            if np.random.rand(1) >= e:\n",
    "                action_index = np.argmax(action_probability)\n",
    "            else:\n",
    "                if (env_name == 'MineRLNavigateDense-v0'):\n",
    "                    action_index = random.randint(0,4)\n",
    "                elif (env_name == 'MineRLTreechop-v0'):\n",
    "                    action_index = random.randint(0,6)\n",
    "        \n",
    "            action = env.action_space.noop()\n",
    "            if (env_name == 'MineRLNavigateDense-v0'):\n",
    "                if (action_index == 0):\n",
    "                    action['camera'] = [0, -10]\n",
    "                    action['jump'] = 0\n",
    "                    action['forward'] = 1\n",
    "                    action['sprint'] = 1\n",
    "                elif (action_index == 1):\n",
    "                    action['camera'] = [0, 10]\n",
    "                    action['jump'] = 0\n",
    "                    action['forward'] = 1\n",
    "                    action['sprint'] = 1\n",
    "                elif (action_index == 2):\n",
    "                    action['camera'] = [0, 0]\n",
    "                    action['jump'] = 0\n",
    "                    action['forward'] = 1\n",
    "                    action['sprint'] = 1\n",
    "                else:\n",
    "                    action['camera'] = [0, 0]\n",
    "                    action['jump'] = 1\n",
    "                    action['forward'] = 1\n",
    "                    action['sprint'] = 1\n",
    "            elif (env_name == 'MineRLTreechop-v0'):\n",
    "                if (action_index == 0):\n",
    "                    action['camera'] = [0, -10]\n",
    "                    action['jump'] = 0\n",
    "                    action['forward'] = 1\n",
    "                    action['attack'] = 1\n",
    "                    action['sprint'] = 0\n",
    "                elif (action_index == 1):\n",
    "                    action['camera'] = [0, 10]\n",
    "                    action['jump'] = 0\n",
    "                    action['forward'] = 1\n",
    "                    action['attack'] = 1\n",
    "                    action['sprint'] = 0\n",
    "                elif (action_index == 2):\n",
    "                    action['camera'] = [-10, 0]\n",
    "                    action['jump'] = 0\n",
    "                    action['forward'] = 1\n",
    "                    action['attack'] = 1\n",
    "                    action['sprint'] = 0\n",
    "                elif (action_index == 3):\n",
    "                    action['camera'] = [10, 0]\n",
    "                    action['jump'] = 0\n",
    "                    action['forward'] = 1\n",
    "                    action['attack'] = 1\n",
    "                    action['sprint'] = 0\n",
    "                elif (action_index == 4):\n",
    "                    action['camera'] = [0, 0]\n",
    "                    action['jump'] = 0\n",
    "                    action['forward'] = 0\n",
    "                    action['attack'] = 1\n",
    "                    action['sprint'] = 0\n",
    "                else:\n",
    "                    action['camera'] = [0, 0]\n",
    "                    action['jump'] = 1\n",
    "                    action['forward'] = 1\n",
    "                    action['attack'] = 1\n",
    "                    action['sprint'] = 0\n",
    "        \n",
    "            action['back'] = 0\n",
    "            action['left'] = 0\n",
    "            action['right'] = 0\n",
    "\n",
    "            obs1, reward, done, info = env.step(action)\n",
    "        \n",
    "            obs_convert = obs['pov'].astype(np.float32) / 255.0 - 0.5\n",
    "            obs1_convert = obs1['pov'].astype(np.float32) / 255.0 - 0.5\n",
    "            ep_history.append([obs_convert, action_index, reward, obs1_convert])\n",
    "            obs = obs1\n",
    "            net_reward += reward\n",
    "        \n",
    "            if done == True:\n",
    "                print(\"Total reward: \", net_reward)\n",
    "                print(\"e: \", e)\n",
    "                \n",
    "                #Update the network.\n",
    "                ep_history = np.array(ep_history)\n",
    "                ep_history[:,2] = discount_rewards(ep_history[:,2])\n",
    "            \n",
    "                #print(\"ep_history[:,0]: \" + str(ep_history[:,0]))\n",
    "                feed_dict={reward_holder:ep_history[:,2],\n",
    "                           action_holder:ep_history[:,1], state:np.stack(ep_history[:,0], 0)}\n",
    "                grads = sess.run(gradients, feed_dict=feed_dict)\n",
    "                for idx, grad in enumerate(grads):\n",
    "                    gradBuffer[idx] += grad\n",
    "\n",
    "                #if i % update_frequency == 0 and i != 0:\n",
    "                feed_dict = dictionary = dict(zip(gradient_holders,gradBuffer))\n",
    "                _ = sess.run(update_batch, feed_dict=feed_dict)\n",
    "                for ix, grad in enumerate(gradBuffer):\n",
    "                    gradBuffer[ix] = grad * 0\n",
    "                \n",
    "                model_path = '/home/kimbring2/MineRL_Git/model/' + env_name\n",
    "                saver.save(sess, model_path + '/model-' + str(i) + '.cptk')\n",
    "                print(\"Saved Model\")\n",
    "                print(\"\")\n",
    "                \n",
    "                total_reward.append(net_reward)\n",
    "                #total_length.append(j)\n",
    "                e = e * 0.999\n",
    "                break\n",
    "        \n",
    "            #Update our running tally of scores.\n",
    "            #if i % 100 == 0:\n",
    "            #print(np.mean(total_reward[-100:]))\n",
    "            #i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "minerl_env",
   "language": "python",
   "name": "minerl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
