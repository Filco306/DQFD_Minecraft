{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Reinforcement Learning in MineRL\n",
    "This tutorial contains a simple example of how to build a Reinforcement Learning based agent that can solve the MineRLNavigateDense-v0 environment. We use the network learned in the previous code [MineRL Imitation Learning](https://github.com/kimbring2/MineRL/blob/master/MineRL_IL_Recurrent.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "try:\n",
    "    xrange = xrange\n",
    "except:\n",
    "    xrange = range\n",
    "    \n",
    "#env_name = 'MineRLNavigateDense-v0'\n",
    "env_name = 'MineRLTreechop-v0'\n",
    "data_path = '/media/kimbring2/6224AA7924AA5039/minerl_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "\n",
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up our Neural Network agent\n",
    "This time we will be using a Convolutional Neural Network that takes observations, passes them through a single hidden layer, and then produces a probability of choosing a Jump and Camera movement. To learn more about this network, see [Convolutional Neural Networks for Visual Recognition Course](http://cs231n.stanford.edu/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kimbring2/minerl_env/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/kimbring2/minerl_env/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /home/kimbring2/minerl_env/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kimbring2/minerl_env/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "H = 1024\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "if (env_name == 'MineRLTreechop-v0'):\n",
    "    state = tf.placeholder(shape=[None,64,64,3], dtype=tf.float32)\n",
    "elif (env_name == 'MineRLNavigateDense-v0'):\n",
    "    state = tf.placeholder(shape=[None,64,64,4], dtype=tf.float32)\n",
    "\n",
    "conv1 = slim.conv2d( \\\n",
    "            inputs=state,num_outputs=32,kernel_size=[8,8],stride=[4,4],padding='VALID', \n",
    "                    biases_initializer=None,activation_fn=tf.nn.relu)\n",
    "conv2 = slim.conv2d( \\\n",
    "            inputs=conv1,num_outputs=64,kernel_size=[4,4],stride=[2,2],padding='VALID', \n",
    "                    biases_initializer=None,activation_fn=tf.nn.relu)\n",
    "conv3 = slim.conv2d( \\\n",
    "            inputs=conv2,num_outputs=64,kernel_size=[3,3],stride=[1,1],padding='VALID', \n",
    "                    biases_initializer=None,activation_fn=tf.nn.relu)\n",
    "\n",
    "convFlat = slim.flatten(conv3)\n",
    "#print(\"convFlat: \" + str(convFlat))\n",
    "\n",
    "if (env_name == 'MineRLTreechop-v0'):\n",
    "    W = tf.get_variable(\"W\", shape=[H,6],\n",
    "               initializer=tf.contrib.layers.xavier_initializer())\n",
    "    score = tf.matmul(convFlat, W)\n",
    "    probability = tf.nn.softmax(score)\n",
    "    #real_action = tf.placeholder(shape=[None,6], dtype=tf.int32)\n",
    "elif (env_name == 'MineRLNavigateDense-v0'):\n",
    "    W = tf.get_variable(\"W\", shape=[H,4],\n",
    "               initializer=tf.contrib.layers.xavier_initializer())\n",
    "    score = tf.matmul(convFlat, W)\n",
    "    probability = tf.nn.softmax(score)\n",
    "    #real_action = tf.placeholder(shape=[None,4], dtype=tf.int32)\n",
    "\n",
    "reward_holder = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "action_holder = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        \n",
    "indexes = tf.range(0, tf.shape(probability)[0]) * tf.shape(probability)[1] + action_holder\n",
    "responsible_outputs = tf.gather(tf.reshape(probability, [-1]), indexes)\n",
    "\n",
    "loss = -tf.reduce_mean(tf.log(responsible_outputs) * reward_holder)\n",
    "    \n",
    "#loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=real_action, \n",
    "#                                                                     logits=score))\n",
    "#tf.summary.scalar('loss', loss)\n",
    "\n",
    "tvars = tf.trainable_variables()\n",
    "gradient_holders = []\n",
    "for idx, var in enumerate(tvars):\n",
    "    placeholder = tf.placeholder(tf.float32, name=str(idx) + '_holder')\n",
    "    gradient_holders.append(placeholder)\n",
    "        \n",
    "gradients = tf.gradients(loss, tvars)\n",
    "        \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "update_batch = optimizer.apply_gradients(zip(gradient_holders,tvars))\n",
    "\n",
    "#train_step = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "#merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    (noop) : 'MineRLNavigateDense-v0'\\n    {'jump': 0, 'camera': [0, 0]} = 3\\n    {'jump': 1, 'camera': [0, 0]} = 4\\n    {'jump': 0, 'camera': [0, -10]} = 1\\n    {'jump': 0, 'camera': [0, 10]} = 2\\n            \\n    (noop) : 'MineRLTreechop-v0'\\n    {'forward': 0, 'jump': 0, 'camera': [0, 0]} = 5\\n    {'forward': 1, 'jump': 1, 'camera': [0, 0]} = 6\\n    {'forward': 1, 'jump': 0, 'camera': [0, -10]} = 1\\n    {'forward': 1, 'jump': 0, 'camera': [0, 10]} = 2\\n    {'forward': 1, 'jump': 0, 'camera': [-10, 0]} = 3\\n    {'forward': 1, 'jump': 0, 'camera': [10, 0]} = 4\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    (noop) : 'MineRLNavigateDense-v0'\n",
    "    {'jump': 0, 'camera': [0, 0]} = 3\n",
    "    {'jump': 1, 'camera': [0, 0]} = 4\n",
    "    {'jump': 0, 'camera': [0, -10]} = 1\n",
    "    {'jump': 0, 'camera': [0, 10]} = 2\n",
    "            \n",
    "    (noop) : 'MineRLTreechop-v0'\n",
    "    {'forward': 0, 'jump': 0, 'camera': [0, 0]} = 5\n",
    "    {'forward': 1, 'jump': 1, 'camera': [0, 0]} = 6\n",
    "    {'forward': 1, 'jump': 0, 'camera': [0, -10]} = 1\n",
    "    {'forward': 1, 'jump': 0, 'camera': [0, 10]} = 2\n",
    "    {'forward': 1, 'jump': 0, 'camera': [-10, 0]} = 3\n",
    "    {'forward': 1, 'jump': 0, 'camera': [10, 0]} = 4\n",
    "''' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n",
      "WARNING:tensorflow:From /home/kimbring2/minerl_env/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from /home/kimbring2/MineRL_Git/model/MineRLTreechop-v0/model-114300.cptk\n",
      "Total reward:  12.0\n",
      "e:  0.05\n",
      "Saved Model\n",
      "\n",
      "Total reward:  7.0\n",
      "e:  0.04995\n",
      "Saved Model\n",
      "\n",
      "Total reward:  17.0\n",
      "e:  0.04990005\n",
      "Saved Model\n",
      "\n",
      "Total reward:  21.0\n",
      "e:  0.04985014995\n",
      "Saved Model\n",
      "\n",
      "Total reward:  7.0\n",
      "e:  0.04980029980005\n",
      "Saved Model\n",
      "\n",
      "Total reward:  15.0\n",
      "e:  0.04975049950024995\n",
      "WARNING:tensorflow:From /home/kimbring2/minerl_env/lib/python3.6/site-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Saved Model\n",
      "\n",
      "Total reward:  17.0\n",
      "e:  0.0497007490007497\n",
      "Saved Model\n",
      "\n",
      "Total reward:  19.0\n",
      "e:  0.04965104825174895\n",
      "Saved Model\n",
      "\n",
      "Total reward:  10.0\n",
      "e:  0.049601397203497204\n",
      "Saved Model\n",
      "\n",
      "Total reward:  8.0\n",
      "e:  0.04955179580629371\n",
      "Saved Model\n",
      "\n",
      "Total reward:  15.0\n",
      "e:  0.04950224401048742\n",
      "Saved Model\n",
      "\n",
      "Total reward:  10.0\n",
      "e:  0.04945274176647693\n",
      "Saved Model\n",
      "\n",
      "Total reward:  24.0\n",
      "e:  0.04940328902471045\n",
      "Saved Model\n",
      "\n",
      "Total reward:  26.0\n",
      "e:  0.04935388573568574\n",
      "Saved Model\n",
      "\n",
      "Total reward:  15.0\n",
      "e:  0.04930453184995005\n",
      "Saved Model\n",
      "\n",
      "Total reward:  5.0\n",
      "e:  0.0492552273181001\n",
      "Saved Model\n",
      "\n",
      "Total reward:  5.0\n",
      "e:  0.049205972090782\n",
      "Saved Model\n",
      "\n",
      "Total reward:  27.0\n",
      "e:  0.04915676611869122\n",
      "Saved Model\n",
      "\n",
      "Total reward:  19.0\n",
      "e:  0.04910760935257253\n",
      "Saved Model\n",
      "\n",
      "Total reward:  9.0\n",
      "e:  0.049058501743219955\n",
      "Saved Model\n",
      "\n",
      "Total reward:  9.0\n",
      "e:  0.049009443241476734\n",
      "Saved Model\n",
      "\n",
      "Total reward:  2.0\n",
      "e:  0.04896043379823526\n",
      "Saved Model\n",
      "\n",
      "Total reward:  17.0\n",
      "e:  0.048911473364437026\n",
      "Saved Model\n",
      "\n",
      "Total reward:  12.0\n",
      "e:  0.04886256189107259\n",
      "Saved Model\n",
      "\n",
      "Total reward:  0.0\n",
      "e:  0.04881369932918152\n",
      "Saved Model\n",
      "\n",
      "Total reward:  19.0\n",
      "e:  0.04876488562985234\n",
      "Saved Model\n",
      "\n",
      "Total reward:  19.0\n",
      "e:  0.048716120744222484\n",
      "Saved Model\n",
      "\n",
      "Total reward:  0.0\n",
      "e:  0.04866740462347826\n",
      "Saved Model\n",
      "\n",
      "Total reward:  9.0\n",
      "e:  0.048618737218854784\n",
      "Saved Model\n",
      "\n",
      "Total reward:  14.0\n",
      "e:  0.04857011848163593\n",
      "Saved Model\n",
      "\n",
      "Total reward:  8.0\n",
      "e:  0.04852154836315429\n",
      "Saved Model\n",
      "\n",
      "Total reward:  6.0\n",
      "e:  0.04847302681479113\n",
      "Saved Model\n",
      "\n",
      "Total reward:  0.0\n",
      "e:  0.04842455378797634\n",
      "Saved Model\n",
      "\n",
      "Total reward:  22.0\n",
      "e:  0.04837612923418836\n",
      "Saved Model\n",
      "\n",
      "Total reward:  7.0\n",
      "e:  0.04832775310495417\n",
      "Saved Model\n",
      "\n",
      "Total reward:  5.0\n",
      "e:  0.04827942535184922\n",
      "Saved Model\n",
      "\n",
      "Total reward:  12.0\n",
      "e:  0.04823114592649737\n",
      "Saved Model\n",
      "\n",
      "Total reward:  12.0\n",
      "e:  0.04818291478057087\n",
      "Saved Model\n",
      "\n",
      "Total reward:  9.0\n",
      "e:  0.0481347318657903\n",
      "Saved Model\n",
      "\n",
      "Total reward:  7.0\n",
      "e:  0.04808659713392451\n",
      "Saved Model\n",
      "\n",
      "Total reward:  24.0\n",
      "e:  0.04803851053679059\n",
      "Saved Model\n",
      "\n",
      "Total reward:  11.0\n",
      "e:  0.0479904720262538\n",
      "Saved Model\n",
      "\n",
      "Total reward:  17.0\n",
      "e:  0.04794248155422754\n",
      "Saved Model\n",
      "\n",
      "Total reward:  18.0\n",
      "e:  0.04789453907267331\n",
      "Saved Model\n",
      "\n",
      "Total reward:  10.0\n",
      "e:  0.04784664453360064\n",
      "Saved Model\n",
      "\n",
      "Total reward:  17.0\n",
      "e:  0.04779879788906704\n",
      "Saved Model\n",
      "\n",
      "Total reward:  11.0\n",
      "e:  0.04775099909117797\n",
      "Saved Model\n",
      "\n",
      "Total reward:  9.0\n",
      "e:  0.04770324809208679\n",
      "Saved Model\n",
      "\n",
      "Total reward:  27.0\n",
      "e:  0.04765554484399471\n",
      "Saved Model\n",
      "\n",
      "Total reward:  8.0\n",
      "e:  0.04760788929915071\n",
      "Saved Model\n",
      "\n",
      "Total reward:  6.0\n",
      "e:  0.04756028140985156\n",
      "Saved Model\n",
      "\n",
      "Total reward:  13.0\n",
      "e:  0.04751272112844171\n",
      "Saved Model\n",
      "\n",
      "Total reward:  12.0\n",
      "e:  0.04746520840731327\n",
      "Saved Model\n",
      "\n",
      "Total reward:  15.0\n",
      "e:  0.04741774319890595\n",
      "Saved Model\n",
      "\n",
      "Total reward:  4.0\n",
      "e:  0.047370325455707046\n",
      "Saved Model\n",
      "\n",
      "Total reward:  13.0\n",
      "e:  0.04732295513025134\n",
      "Saved Model\n",
      "\n",
      "Total reward:  4.0\n",
      "e:  0.04727563217512109\n",
      "Saved Model\n",
      "\n",
      "Total reward:  10.0\n",
      "e:  0.047228356542945965\n",
      "Saved Model\n",
      "\n",
      "Total reward:  13.0\n",
      "e:  0.04718112818640302\n",
      "Saved Model\n",
      "\n",
      "Total reward:  12.0\n",
      "e:  0.047133947058216615\n",
      "Saved Model\n",
      "\n",
      "Total reward:  9.0\n",
      "e:  0.0470868131111584\n",
      "Saved Model\n",
      "\n",
      "Total reward:  0.0\n",
      "e:  0.047039726298047244\n",
      "Saved Model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import minerl\n",
    "import gym\n",
    "import os\n",
    "import random\n",
    "from env_wrappers import ContinuingTimeLimitMonitor\n",
    "\n",
    "env = gym.make(env_name)\n",
    "#env = ContinuingTimeLimitMonitor(env, os.path.join('/home/kimbring2/MineRL/', 'monitor'), mode='evaluation', \n",
    "#                                 video_callable=lambda episode_id: True)\n",
    "\n",
    "e = 0.05\n",
    "update_frequency = 100\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "# Launch the graph\n",
    "    sess.run(init)\n",
    "    saver = tf.train.Saver(max_to_keep=5)\n",
    "    \n",
    "    print('Loading Model...') \n",
    "    path = '/home/kimbring2/MineRL_Git/model/' + env_name\n",
    "    ckpt = tf.train.get_checkpoint_state(path)\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    \n",
    "    for i in range(5000):\n",
    "        total_reward = []\n",
    "        total_length = []\n",
    "    \n",
    "        gradBuffer = sess.run(tf.trainable_variables())\n",
    "        for ix, grad in enumerate(gradBuffer):\n",
    "            gradBuffer[ix] = grad * 0\n",
    "    \n",
    "        env.init()\n",
    "        obs = env.reset()\n",
    "        net_reward = 0\n",
    "        ep_history = []\n",
    "        while True:\n",
    "            if (env_name == 'MineRLTreechop-v0'):\n",
    "                state_concat = obs['pov'].astype(np.float32) / 255.0 - 0.5\n",
    "            elif (env_name == 'MineRLNavigateDense-v0'):\n",
    "                pov = obs['pov'].astype(np.float32) / 255.0 - 0.5\n",
    "                compass = obs['compassAngle']\n",
    "                compass_channel = np.ones(shape=list(pov.shape[:-1]) + [1], dtype=np.float32) * compass\n",
    "                compass_channel /= 180.0\n",
    "                state_concat = np.concatenate([pov, compass_channel], axis=-1)\n",
    "            \n",
    "            action_probability = sess.run(probability, feed_dict={state:[state_concat]})\n",
    "            if np.random.rand(1) >= e:\n",
    "                action_index = np.argmax(action_probability)\n",
    "            else:\n",
    "                if (env_name == 'MineRLNavigateDense-v0'):\n",
    "                    action_index = random.randint(0,4)\n",
    "                elif (env_name == 'MineRLTreechop-v0'):\n",
    "                    action_index = random.randint(0,6)\n",
    "        \n",
    "            action = env.action_space.noop()\n",
    "            if (env_name == 'MineRLNavigateDense-v0'):\n",
    "                if (action_index == 0):\n",
    "                    action['camera'] = [0, -10]\n",
    "                    action['jump'] = 0\n",
    "                    action['forward'] = 1\n",
    "                    action['sprint'] = 1\n",
    "                elif (action_index == 1):\n",
    "                    action['camera'] = [0, 10]\n",
    "                    action['jump'] = 0\n",
    "                    action['forward'] = 1\n",
    "                    action['sprint'] = 1\n",
    "                elif (action_index == 2):\n",
    "                    action['camera'] = [0, 0]\n",
    "                    action['jump'] = 0\n",
    "                    action['forward'] = 1\n",
    "                    action['sprint'] = 1\n",
    "                else:\n",
    "                    action['camera'] = [0, 0]\n",
    "                    action['jump'] = 1\n",
    "                    action['forward'] = 1\n",
    "                    action['sprint'] = 1\n",
    "            elif (env_name == 'MineRLTreechop-v0'):\n",
    "                if (action_index == 0):\n",
    "                    action['camera'] = [0, -10]\n",
    "                    action['jump'] = 0\n",
    "                    action['forward'] = 1\n",
    "                    action['attack'] = 1\n",
    "                    action['sprint'] = 0\n",
    "                elif (action_index == 1):\n",
    "                    action['camera'] = [0, 10]\n",
    "                    action['jump'] = 0\n",
    "                    action['forward'] = 1\n",
    "                    action['attack'] = 1\n",
    "                    action['sprint'] = 0\n",
    "                elif (action_index == 2):\n",
    "                    action['camera'] = [-10, 0]\n",
    "                    action['jump'] = 0\n",
    "                    action['forward'] = 1\n",
    "                    action['attack'] = 1\n",
    "                    action['sprint'] = 0\n",
    "                elif (action_index == 3):\n",
    "                    action['camera'] = [10, 0]\n",
    "                    action['jump'] = 0\n",
    "                    action['forward'] = 1\n",
    "                    action['attack'] = 1\n",
    "                    action['sprint'] = 0\n",
    "                elif (action_index == 4):\n",
    "                    action['camera'] = [0, 0]\n",
    "                    action['jump'] = 0\n",
    "                    action['forward'] = 0\n",
    "                    action['attack'] = 1\n",
    "                    action['sprint'] = 0\n",
    "                else:\n",
    "                    action['camera'] = [0, 0]\n",
    "                    action['jump'] = 1\n",
    "                    action['forward'] = 1\n",
    "                    action['attack'] = 1\n",
    "                    action['sprint'] = 0\n",
    "        \n",
    "            action['back'] = 0\n",
    "            action['left'] = 0\n",
    "            action['right'] = 0\n",
    "\n",
    "            obs1, reward, done, info = env.step(action)\n",
    "        \n",
    "            obs_convert = obs['pov'].astype(np.float32) / 255.0 - 0.5\n",
    "            obs1_convert = obs1['pov'].astype(np.float32) / 255.0 - 0.5\n",
    "            ep_history.append([obs_convert, action_index, reward, obs1_convert])\n",
    "            obs = obs1\n",
    "            net_reward += reward\n",
    "        \n",
    "            if done == True:\n",
    "                print(\"Total reward: \", net_reward)\n",
    "                print(\"e: \", e)\n",
    "                \n",
    "                #Update the network.\n",
    "                ep_history = np.array(ep_history)\n",
    "                ep_history[:,2] = discount_rewards(ep_history[:,2])\n",
    "            \n",
    "                #print(\"ep_history[:,0]: \" + str(ep_history[:,0]))\n",
    "                feed_dict={reward_holder:ep_history[:,2],\n",
    "                           action_holder:ep_history[:,1], state:np.stack(ep_history[:,0], 0)}\n",
    "                grads = sess.run(gradients, feed_dict=feed_dict)\n",
    "                for idx, grad in enumerate(grads):\n",
    "                    gradBuffer[idx] += grad\n",
    "\n",
    "                #if i % update_frequency == 0 and i != 0:\n",
    "                feed_dict = dictionary = dict(zip(gradient_holders,gradBuffer))\n",
    "                _ = sess.run(update_batch, feed_dict=feed_dict)\n",
    "                for ix, grad in enumerate(gradBuffer):\n",
    "                    gradBuffer[ix] = grad * 0\n",
    "                \n",
    "                model_path = '/home/kimbring2/MineRL_Git/model/' + env_name\n",
    "                saver.save(sess, model_path + '/model-' + str(i) + '.cptk')\n",
    "                print(\"Saved Model\")\n",
    "                print(\"\")\n",
    "                \n",
    "                total_reward.append(net_reward)\n",
    "                #total_length.append(j)\n",
    "                e = e * 0.999\n",
    "                break\n",
    "        \n",
    "            #Update our running tally of scores.\n",
    "            #if i % 100 == 0:\n",
    "            #print(np.mean(total_reward[-100:]))\n",
    "            #i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "minerl_env",
   "language": "python",
   "name": "minerl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
